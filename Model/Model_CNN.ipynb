{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb#scrollTo=lvDVFkg-2DPm -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando um exemplo de CNN de previsão de valores \n",
    "\n",
    "Predicao de imagens:\n",
    "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb#scrollTo=lvDVFkg-2DPm\n",
    "\n",
    "\n",
    "Tuning hyperparameters\n",
    "https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import keras_tuner as kt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo Dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60%\n",
    "PATH_TREINO = '../Data/3_Gold/Treino_all_stocks.csv'\n",
    "DF_TREINO = pd.read_csv(PATH_TREINO, sep=\",\")\n",
    "train_dataset = DF_TREINO\n",
    "train_dataset = train_dataset.drop(['oil_5','usd_5','abev_5','jbs_5','petr_5','vale_5',], axis=1)\n",
    "train_labels = train_dataset.pop('ibova_5')\n",
    "\n",
    "# 20%\n",
    "PATH_VALIDACAO = '../Data/3_Gold/Validacao_all_stocks.csv'\n",
    "DF_VALIDACAO = pd.read_csv(PATH_VALIDACAO, sep=\",\")\n",
    "valid_dataset = DF_VALIDACAO\n",
    "valid_dataset = valid_dataset.drop(['ibova_5','oil_5','usd_5','abev_5','jbs_5','petr_5','vale_5',],axis=1)\n",
    "# valid_labels = valid_dataset.pop('ibova_5') \n",
    "\n",
    "# 20%\n",
    "PATH_TESTE = '../Data/3_Gold/Teste_all_stocks.csv'\n",
    "DF_TESTE = pd.read_csv(PATH_TESTE, sep=\",\")\n",
    "test_dataset = DF_TESTE\n",
    "test_dataset = test_dataset.drop(['oil_5','usd_5','abev_5','jbs_5','petr_5','vale_5',],axis=1)\n",
    "test_labels = test_dataset.pop('ibova_5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecionando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(train_dataset[['index','ibova_0','ibova_1','ibova_2','ibova_3','ibova_4','oil_0','oil_1','oil_2','oil_3','oil_4','usd_0','usd_1','usd_2','usd_3','usd_4']], diag_kind=\"kde\")\n",
    "# sns.pairplot(train_dataset[['index','ibova_0','oil_0','usd_0']], diag_kind=\"kde\")\n",
    "# sns.pairplot(train_dataset[['ibova_1','oil_1','usd_1']], diag_kind=\"kde\")\n",
    "\n",
    "train_stats = train_dataset.describe()\n",
    "# train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    def nn_cl_fun():\n",
    "        opt = Adam(lr = learning_rate)\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "        nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_l,en, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def model_builder(hp):\n",
    "  hp_unit_1 = hp.Int('units', min_value=16, max_value=64, step=1)\n",
    "  hp_unit_2 = hp.Int('units', min_value=4, max_value=64, step=1)\n",
    "  hp_unit_3 = hp.Int('units', min_value=4, max_value=64, step=1)\n",
    "  hp_unit_4 = hp.Int('units', min_value=4, max_value=64, step=1)\n",
    "\n",
    "  model = keras.Sequential([\n",
    "    layers.Conv2D(units=hp_unit_1, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
    "    layers.Dense(units=hp_unit_2, activation='relu'),\n",
    "    layers.Dense(units=hp_unit_3, activation='relu'),\n",
    "    layers.Dense(units=hp_unit_4, activation='relu'),\n",
    "    layers.Dense(units=1, activation='linear')\n",
    "  ])\n",
    "\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mse',\n",
    "                metrics=['mae', 'mse'])\n",
    "\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "# rmse pra avaliação, nao para loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 44)                1628      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 44)                1980      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 44)                1980      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 44)                1980      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 45        \n",
      "=================================================================\n",
      "Total params: 7,613\n",
      "Trainable params: 7,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciando Tuner\n",
    "\n",
    "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb#scrollTo=McO82AXOuxXh\n",
    "\n",
    "### extra hipertuner\n",
    "<!-- https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project my_dir\\intro_to_kt\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from my_dir\\intro_to_kt\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_mse',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='logs',\n",
    "                     project_name='hyper_parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 44 and the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# early_stop = keras.callbacks.EarlyStopping(\n",
    "#   monitor='val_mse', patience=50, mode='min' ,restore_best_weights=True)\n",
    "\t\n",
    "EPOCHS = 500\n",
    "\n",
    "tuner.search(train_dataset, train_labels, epochs=EPOCHS, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=20)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecionando a melhor epoca do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 91.7225 - mae: 3.8343 - mse: 91.7225 - val_loss: 0.2838 - val_mae: 0.4372 - val_mse: 0.2838\n",
      "Epoch 2/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3150 - mae: 0.4421 - mse: 0.3150 - val_loss: 0.7613 - val_mae: 0.7951 - val_mse: 0.7613\n",
      "Epoch 3/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4350 - mae: 0.4864 - mse: 0.4350 - val_loss: 0.5265 - val_mae: 0.6556 - val_mse: 0.5265\n",
      "Epoch 4/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7430 - mae: 0.5879 - mse: 0.7430 - val_loss: 3.3850 - val_mae: 1.8059 - val_mse: 3.3850\n",
      "Epoch 5/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2342 - mae: 0.3728 - mse: 0.2342 - val_loss: 0.0974 - val_mae: 0.2503 - val_mse: 0.0974\n",
      "Epoch 6/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0987 - mae: 0.2412 - mse: 0.0987 - val_loss: 0.5454 - val_mae: 0.6766 - val_mse: 0.5454\n",
      "Epoch 7/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1012 - mae: 0.2469 - mse: 0.1012 - val_loss: 0.0819 - val_mae: 0.2300 - val_mse: 0.0819\n",
      "Epoch 8/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1011 - mae: 0.2448 - mse: 0.1011 - val_loss: 0.1359 - val_mae: 0.2951 - val_mse: 0.1359\n",
      "Epoch 9/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0880 - mae: 0.2273 - mse: 0.0880 - val_loss: 0.2449 - val_mae: 0.4317 - val_mse: 0.2449\n",
      "Epoch 10/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1365 - mae: 0.2801 - mse: 0.1365 - val_loss: 0.1186 - val_mae: 0.2803 - val_mse: 0.1186\n",
      "Epoch 11/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1820 - mae: 0.3288 - mse: 0.1820 - val_loss: 0.7170 - val_mae: 0.8017 - val_mse: 0.7170\n",
      "Epoch 12/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1282 - mae: 0.2748 - mse: 0.1282 - val_loss: 0.0874 - val_mae: 0.2329 - val_mse: 0.0874\n",
      "Epoch 13/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0947 - mae: 0.2372 - mse: 0.0947 - val_loss: 0.0753 - val_mae: 0.2140 - val_mse: 0.0753\n",
      "Epoch 14/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4882 - mae: 0.5351 - mse: 0.4882 - val_loss: 2.5138 - val_mae: 1.5564 - val_mse: 2.5138\n",
      "Epoch 15/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2796 - mae: 0.4146 - mse: 0.2796 - val_loss: 0.3949 - val_mae: 0.5686 - val_mse: 0.3949\n",
      "Epoch 16/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1160 - mae: 0.2598 - mse: 0.1160 - val_loss: 0.3309 - val_mae: 0.5092 - val_mse: 0.3309\n",
      "Epoch 17/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1132 - mae: 0.2625 - mse: 0.1132 - val_loss: 0.2675 - val_mae: 0.4507 - val_mse: 0.2675\n",
      "Epoch 18/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1317 - mae: 0.2778 - mse: 0.1317 - val_loss: 0.3234 - val_mae: 0.5111 - val_mse: 0.3234\n",
      "Epoch 19/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1249 - mae: 0.2691 - mse: 0.1249 - val_loss: 0.6294 - val_mae: 0.7449 - val_mse: 0.6294\n",
      "Epoch 20/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1643 - mae: 0.3076 - mse: 0.1643 - val_loss: 0.2851 - val_mae: 0.4684 - val_mse: 0.2851\n",
      "Epoch 21/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.2023 - mae: 0.6383 - mse: 1.2023 - val_loss: 11.4169 - val_mae: 3.3547 - val_mse: 11.4169\n",
      "Epoch 22/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4048 - mae: 0.4525 - mse: 0.4048 - val_loss: 0.1023 - val_mae: 0.2577 - val_mse: 0.1023\n",
      "Epoch 23/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0715 - mae: 0.2034 - mse: 0.0715 - val_loss: 0.0892 - val_mae: 0.2385 - val_mse: 0.0892\n",
      "Epoch 24/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0652 - mae: 0.1952 - mse: 0.0652 - val_loss: 0.0825 - val_mae: 0.2279 - val_mse: 0.0825\n",
      "Epoch 25/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0659 - mae: 0.1970 - mse: 0.0659 - val_loss: 0.1572 - val_mae: 0.3272 - val_mse: 0.1572\n",
      "Epoch 26/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0658 - mae: 0.1983 - mse: 0.0658 - val_loss: 0.1096 - val_mae: 0.2686 - val_mse: 0.1096\n",
      "Epoch 27/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0573 - mae: 0.1849 - mse: 0.0573 - val_loss: 0.0904 - val_mae: 0.2330 - val_mse: 0.0904\n",
      "Epoch 28/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0596 - mae: 0.1873 - mse: 0.0596 - val_loss: 0.0753 - val_mae: 0.2123 - val_mse: 0.0753\n",
      "Epoch 29/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0604 - mae: 0.1868 - mse: 0.0604 - val_loss: 0.0741 - val_mae: 0.2103 - val_mse: 0.0741\n",
      "Epoch 30/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0642 - mae: 0.1943 - mse: 0.0642 - val_loss: 0.0854 - val_mae: 0.2330 - val_mse: 0.0854\n",
      "Epoch 31/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0656 - mae: 0.1988 - mse: 0.0656 - val_loss: 0.0745 - val_mae: 0.2135 - val_mse: 0.0745\n",
      "Epoch 32/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0601 - mae: 0.1878 - mse: 0.0601 - val_loss: 0.1101 - val_mae: 0.2728 - val_mse: 0.1101\n",
      "Epoch 33/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0594 - mae: 0.1889 - mse: 0.0594 - val_loss: 0.1303 - val_mae: 0.2996 - val_mse: 0.1303\n",
      "Epoch 34/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0577 - mae: 0.1847 - mse: 0.0577 - val_loss: 0.1332 - val_mae: 0.2957 - val_mse: 0.1332\n",
      "Epoch 35/500\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.0601 - mae: 0.1901 - mse: 0.0601 - val_loss: 0.0753 - val_mae: 0.2117 - val_mse: 0.0753\n",
      "Epoch 36/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0569 - mae: 0.1840 - mse: 0.0569 - val_loss: 0.1273 - val_mae: 0.2890 - val_mse: 0.1273\n",
      "Epoch 37/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0573 - mae: 0.1818 - mse: 0.0573 - val_loss: 0.0986 - val_mae: 0.2459 - val_mse: 0.0986\n",
      "Epoch 38/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0564 - mae: 0.1820 - mse: 0.0564 - val_loss: 0.0748 - val_mae: 0.2132 - val_mse: 0.0748\n",
      "Epoch 39/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0610 - mae: 0.1916 - mse: 0.0610 - val_loss: 0.1374 - val_mae: 0.3088 - val_mse: 0.1374\n",
      "Epoch 40/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0664 - mae: 0.2012 - mse: 0.0664 - val_loss: 0.0744 - val_mae: 0.2101 - val_mse: 0.0744\n",
      "Epoch 41/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0602 - mae: 0.1899 - mse: 0.0602 - val_loss: 0.0763 - val_mae: 0.2151 - val_mse: 0.0763\n",
      "Epoch 42/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0573 - mae: 0.1844 - mse: 0.0573 - val_loss: 0.1499 - val_mae: 0.3241 - val_mse: 0.1499\n",
      "Epoch 43/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0665 - mae: 0.1995 - mse: 0.0665 - val_loss: 0.1111 - val_mae: 0.2680 - val_mse: 0.1111\n",
      "Epoch 44/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0601 - mae: 0.1902 - mse: 0.0601 - val_loss: 0.1831 - val_mae: 0.3641 - val_mse: 0.1831\n",
      "Epoch 45/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0561 - mae: 0.1831 - mse: 0.0561 - val_loss: 0.1192 - val_mae: 0.2789 - val_mse: 0.1192\n",
      "Epoch 46/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0568 - mae: 0.1840 - mse: 0.0568 - val_loss: 0.0851 - val_mae: 0.2263 - val_mse: 0.0851\n",
      "Epoch 47/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0543 - mae: 0.1799 - mse: 0.0543 - val_loss: 0.0785 - val_mae: 0.2203 - val_mse: 0.0785\n",
      "Epoch 48/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0567 - mae: 0.1837 - mse: 0.0567 - val_loss: 0.0856 - val_mae: 0.2277 - val_mse: 0.0856\n",
      "Epoch 49/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0598 - mae: 0.1883 - mse: 0.0598 - val_loss: 0.1470 - val_mae: 0.3169 - val_mse: 0.1470\n",
      "Epoch 50/500\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.0628 - mae: 0.1911 - mse: 0.0628 - val_loss: 0.0977 - val_mae: 0.2471 - val_mse: 0.0977\n",
      "Epoch 51/500\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.0669 - mae: 0.1962 - mse: 0.066 - 0s 6ms/step - loss: 0.0651 - mae: 0.1967 - mse: 0.0651 - val_loss: 0.2562 - val_mae: 0.4453 - val_mse: 0.2562\n",
      "Epoch 52/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0576 - mae: 0.1847 - mse: 0.0576 - val_loss: 0.0717 - val_mae: 0.2080 - val_mse: 0.0717\n",
      "Epoch 53/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0541 - mae: 0.1780 - mse: 0.0541 - val_loss: 0.0759 - val_mae: 0.2127 - val_mse: 0.0759\n",
      "Epoch 54/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0593 - mae: 0.1884 - mse: 0.0593 - val_loss: 0.0725 - val_mae: 0.2091 - val_mse: 0.0725\n",
      "Epoch 55/500\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.0531 - mae: 0.1770 - mse: 0.0531 - val_loss: 0.0857 - val_mae: 0.2277 - val_mse: 0.0857\n",
      "Epoch 56/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0567 - mae: 0.1835 - mse: 0.0567 - val_loss: 0.0892 - val_mae: 0.2384 - val_mse: 0.0892\n",
      "Epoch 57/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0534 - mae: 0.1777 - mse: 0.0534 - val_loss: 0.0784 - val_mae: 0.2163 - val_mse: 0.0784\n",
      "Epoch 58/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0538 - mae: 0.1772 - mse: 0.0538 - val_loss: 0.0747 - val_mae: 0.2114 - val_mse: 0.0747\n",
      "Epoch 59/500\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 0.0567 - mae: 0.1823 - mse: 0.0567 - val_loss: 0.0737 - val_mae: 0.2101 - val_mse: 0.0737\n",
      "Epoch 60/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0546 - mae: 0.1782 - mse: 0.0546 - val_loss: 0.0836 - val_mae: 0.2295 - val_mse: 0.0836\n",
      "Epoch 61/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0599 - mae: 0.1870 - mse: 0.0599 - val_loss: 0.1507 - val_mae: 0.3254 - val_mse: 0.1507\n",
      "Epoch 62/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0658 - mae: 0.1980 - mse: 0.0658 - val_loss: 0.0738 - val_mae: 0.2092 - val_mse: 0.0738\n",
      "Epoch 63/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0538 - mae: 0.1794 - mse: 0.0538 - val_loss: 0.0766 - val_mae: 0.2150 - val_mse: 0.0766\n",
      "Epoch 64/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0553 - mae: 0.1819 - mse: 0.0553 - val_loss: 0.0772 - val_mae: 0.2179 - val_mse: 0.0772\n",
      "Epoch 65/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0555 - mae: 0.1799 - mse: 0.0555 - val_loss: 0.0767 - val_mae: 0.2179 - val_mse: 0.0767\n",
      "Epoch 66/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0697 - mae: 0.2035 - mse: 0.0697 - val_loss: 0.0819 - val_mae: 0.2251 - val_mse: 0.0819\n",
      "Epoch 67/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0597 - mae: 0.1880 - mse: 0.0597 - val_loss: 0.1324 - val_mae: 0.2986 - val_mse: 0.1324\n",
      "Epoch 68/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0541 - mae: 0.1788 - mse: 0.0541 - val_loss: 0.1159 - val_mae: 0.2763 - val_mse: 0.1159\n",
      "Epoch 69/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0546 - mae: 0.1806 - mse: 0.0546 - val_loss: 0.0762 - val_mae: 0.2145 - val_mse: 0.0762\n",
      "Epoch 70/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0536 - mae: 0.1785 - mse: 0.0536 - val_loss: 0.0733 - val_mae: 0.2102 - val_mse: 0.0733\n",
      "Epoch 71/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0562 - mae: 0.1818 - mse: 0.0562 - val_loss: 0.0740 - val_mae: 0.2138 - val_mse: 0.0740\n",
      "Epoch 72/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0534 - mae: 0.1792 - mse: 0.0534 - val_loss: 0.0753 - val_mae: 0.2123 - val_mse: 0.0753\n",
      "Epoch 73/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0577 - mae: 0.1828 - mse: 0.0577 - val_loss: 0.1149 - val_mae: 0.2743 - val_mse: 0.1149\n",
      "Epoch 74/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0613 - mae: 0.1881 - mse: 0.0613 - val_loss: 0.1556 - val_mae: 0.3333 - val_mse: 0.1556\n",
      "Epoch 75/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0535 - mae: 0.1779 - mse: 0.0535 - val_loss: 0.0993 - val_mae: 0.2562 - val_mse: 0.0993\n",
      "Epoch 76/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0538 - mae: 0.1791 - mse: 0.0538 - val_loss: 0.0815 - val_mae: 0.2257 - val_mse: 0.0815\n",
      "Epoch 77/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0525 - mae: 0.1779 - mse: 0.0525 - val_loss: 0.0734 - val_mae: 0.2104 - val_mse: 0.0734\n",
      "Epoch 78/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0549 - mae: 0.1787 - mse: 0.0549 - val_loss: 0.0856 - val_mae: 0.2274 - val_mse: 0.0856\n",
      "Epoch 79/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0539 - mae: 0.1795 - mse: 0.0539 - val_loss: 0.0765 - val_mae: 0.2163 - val_mse: 0.0765\n",
      "Epoch 80/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0521 - mae: 0.1751 - mse: 0.0521 - val_loss: 0.0869 - val_mae: 0.2348 - val_mse: 0.0869\n",
      "Epoch 81/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0557 - mae: 0.1809 - mse: 0.0557 - val_loss: 0.0741 - val_mae: 0.2136 - val_mse: 0.0741\n",
      "Epoch 82/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0528 - mae: 0.1762 - mse: 0.0528 - val_loss: 0.0748 - val_mae: 0.2139 - val_mse: 0.0748\n",
      "Epoch 83/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0525 - mae: 0.1752 - mse: 0.0525 - val_loss: 0.0786 - val_mae: 0.2168 - val_mse: 0.0786\n",
      "Epoch 84/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0574 - mae: 0.1844 - mse: 0.0574 - val_loss: 0.0928 - val_mae: 0.2391 - val_mse: 0.0928\n",
      "Epoch 85/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0616 - mae: 0.1914 - mse: 0.0616 - val_loss: 0.1425 - val_mae: 0.3168 - val_mse: 0.1425\n",
      "Epoch 86/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0561 - mae: 0.1831 - mse: 0.0561 - val_loss: 0.0801 - val_mae: 0.2237 - val_mse: 0.0801\n",
      "Epoch 87/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0588 - mae: 0.1852 - mse: 0.0588 - val_loss: 0.0756 - val_mae: 0.2151 - val_mse: 0.0756\n",
      "Epoch 88/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0537 - mae: 0.1763 - mse: 0.0537 - val_loss: 0.0796 - val_mae: 0.2236 - val_mse: 0.0796\n",
      "Epoch 89/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0524 - mae: 0.1738 - mse: 0.0524 - val_loss: 0.0731 - val_mae: 0.2095 - val_mse: 0.0731\n",
      "Epoch 90/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0503 - mae: 0.1716 - mse: 0.0503 - val_loss: 0.1365 - val_mae: 0.3105 - val_mse: 0.1365\n",
      "Epoch 91/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0597 - mae: 0.1880 - mse: 0.0597 - val_loss: 0.0822 - val_mae: 0.2221 - val_mse: 0.0822\n",
      "Epoch 92/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0511 - mae: 0.1727 - mse: 0.0511 - val_loss: 0.0947 - val_mae: 0.2429 - val_mse: 0.0947\n",
      "Epoch 93/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0592 - mae: 0.1882 - mse: 0.0592 - val_loss: 0.0717 - val_mae: 0.2044 - val_mse: 0.0717\n",
      "Epoch 94/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0527 - mae: 0.1775 - mse: 0.0527 - val_loss: 0.0781 - val_mae: 0.2188 - val_mse: 0.0781\n",
      "Epoch 95/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0506 - mae: 0.1717 - mse: 0.0506 - val_loss: 0.0730 - val_mae: 0.2067 - val_mse: 0.0730\n",
      "Epoch 96/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0551 - mae: 0.1812 - mse: 0.0551 - val_loss: 0.0715 - val_mae: 0.2072 - val_mse: 0.0715\n",
      "Epoch 97/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0535 - mae: 0.1785 - mse: 0.0535 - val_loss: 0.0755 - val_mae: 0.2154 - val_mse: 0.0755\n",
      "Epoch 98/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0523 - mae: 0.1760 - mse: 0.0523 - val_loss: 0.0790 - val_mae: 0.2223 - val_mse: 0.0790\n",
      "Epoch 99/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0595 - mae: 0.1848 - mse: 0.0595 - val_loss: 0.1200 - val_mae: 0.2881 - val_mse: 0.1200\n",
      "Epoch 100/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0554 - mae: 0.1804 - mse: 0.0554 - val_loss: 0.0737 - val_mae: 0.2087 - val_mse: 0.0737\n",
      "Epoch 101/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0542 - mae: 0.1789 - mse: 0.0542 - val_loss: 0.1061 - val_mae: 0.2611 - val_mse: 0.1061\n",
      "Epoch 102/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0525 - mae: 0.1752 - mse: 0.0525 - val_loss: 0.0900 - val_mae: 0.2409 - val_mse: 0.0900\n",
      "Epoch 103/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0546 - mae: 0.1793 - mse: 0.0546 - val_loss: 0.1689 - val_mae: 0.3436 - val_mse: 0.1689\n",
      "Epoch 104/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0538 - mae: 0.1767 - mse: 0.0538 - val_loss: 0.0792 - val_mae: 0.2230 - val_mse: 0.0792\n",
      "Epoch 105/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0516 - mae: 0.1747 - mse: 0.0516 - val_loss: 0.0749 - val_mae: 0.2134 - val_mse: 0.0749\n",
      "Epoch 106/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0501 - mae: 0.1719 - mse: 0.0501 - val_loss: 0.0727 - val_mae: 0.2086 - val_mse: 0.0727\n",
      "Epoch 107/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0502 - mae: 0.1723 - mse: 0.0502 - val_loss: 0.0953 - val_mae: 0.2489 - val_mse: 0.0953\n",
      "Epoch 108/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0605 - mae: 0.1885 - mse: 0.0605 - val_loss: 0.1737 - val_mae: 0.3550 - val_mse: 0.1737\n",
      "Epoch 109/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0583 - mae: 0.1876 - mse: 0.0583 - val_loss: 0.0920 - val_mae: 0.2431 - val_mse: 0.0920\n",
      "Epoch 110/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0534 - mae: 0.1749 - mse: 0.0534 - val_loss: 0.0771 - val_mae: 0.2199 - val_mse: 0.0771\n",
      "Epoch 111/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0519 - mae: 0.1754 - mse: 0.0519 - val_loss: 0.0698 - val_mae: 0.2042 - val_mse: 0.0698\n",
      "Epoch 112/500\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 0.0749 - mae: 0.1995 - mse: 0.0749 - val_loss: 0.2074 - val_mae: 0.3848 - val_mse: 0.2074\n",
      "Epoch 113/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0680 - mae: 0.1990 - mse: 0.0680 - val_loss: 0.0719 - val_mae: 0.2091 - val_mse: 0.0719\n",
      "Epoch 114/500\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.0763 - mae: 0.2110 - mse: 0.0763 - val_loss: 0.1167 - val_mae: 0.2730 - val_mse: 0.1167\n",
      "Epoch 115/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0683 - mae: 0.1995 - mse: 0.0683 - val_loss: 0.1050 - val_mae: 0.2607 - val_mse: 0.1050\n",
      "Epoch 116/500\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.0571 - mae: 0.1833 - mse: 0.0571 - val_loss: 0.0714 - val_mae: 0.2049 - val_mse: 0.0714\n",
      "Epoch 117/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0571 - mae: 0.1832 - mse: 0.0571 - val_loss: 0.1415 - val_mae: 0.3153 - val_mse: 0.1415\n",
      "Epoch 118/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0568 - mae: 0.1849 - mse: 0.0568 - val_loss: 0.1060 - val_mae: 0.2606 - val_mse: 0.1060\n",
      "Epoch 119/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0538 - mae: 0.1792 - mse: 0.0538 - val_loss: 0.0940 - val_mae: 0.2439 - val_mse: 0.0940\n",
      "Epoch 120/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0625 - mae: 0.1946 - mse: 0.0625 - val_loss: 0.1459 - val_mae: 0.3208 - val_mse: 0.1459\n",
      "Epoch 121/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0573 - mae: 0.1833 - mse: 0.0573 - val_loss: 0.0778 - val_mae: 0.2148 - val_mse: 0.0778\n",
      "Epoch 122/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0632 - mae: 0.1939 - mse: 0.0632 - val_loss: 0.0820 - val_mae: 0.2217 - val_mse: 0.0820\n",
      "Epoch 123/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0567 - mae: 0.1818 - mse: 0.0567 - val_loss: 0.1260 - val_mae: 0.2941 - val_mse: 0.1260\n",
      "Epoch 124/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0576 - mae: 0.1849 - mse: 0.0576 - val_loss: 0.1094 - val_mae: 0.2671 - val_mse: 0.1094\n",
      "Epoch 125/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0549 - mae: 0.1806 - mse: 0.0549 - val_loss: 0.1054 - val_mae: 0.2611 - val_mse: 0.1054\n",
      "Epoch 126/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0532 - mae: 0.1770 - mse: 0.0532 - val_loss: 0.0805 - val_mae: 0.2235 - val_mse: 0.0805\n",
      "Epoch 127/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0502 - mae: 0.1717 - mse: 0.0502 - val_loss: 0.0816 - val_mae: 0.2186 - val_mse: 0.0816\n",
      "Epoch 128/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0554 - mae: 0.1798 - mse: 0.0554 - val_loss: 0.0778 - val_mae: 0.2164 - val_mse: 0.0778\n",
      "Epoch 129/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0584 - mae: 0.1854 - mse: 0.0584 - val_loss: 0.1076 - val_mae: 0.2633 - val_mse: 0.1076\n",
      "Epoch 130/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0539 - mae: 0.1795 - mse: 0.0539 - val_loss: 0.2948 - val_mae: 0.4797 - val_mse: 0.2948\n",
      "Epoch 131/500\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0798 - mae: 0.2147 - mse: 0.0798 - val_loss: 0.1234 - val_mae: 0.2869 - val_mse: 0.1234\n",
      "Epoch 132/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0588 - mae: 0.1854 - mse: 0.0588 - val_loss: 0.0775 - val_mae: 0.2194 - val_mse: 0.0775\n",
      "Epoch 133/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0506 - mae: 0.1716 - mse: 0.0506 - val_loss: 0.0716 - val_mae: 0.2083 - val_mse: 0.0716\n",
      "Epoch 134/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0504 - mae: 0.1726 - mse: 0.0504 - val_loss: 0.1011 - val_mae: 0.2584 - val_mse: 0.1011\n",
      "Epoch 135/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0500 - mae: 0.1716 - mse: 0.0500 - val_loss: 0.0814 - val_mae: 0.2220 - val_mse: 0.0814\n",
      "Epoch 136/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0501 - mae: 0.1722 - mse: 0.0501 - val_loss: 0.0724 - val_mae: 0.2100 - val_mse: 0.0724\n",
      "Epoch 137/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0510 - mae: 0.1731 - mse: 0.0510 - val_loss: 0.0711 - val_mae: 0.2057 - val_mse: 0.0711\n",
      "Epoch 138/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0589 - mae: 0.1865 - mse: 0.0589 - val_loss: 0.1651 - val_mae: 0.3437 - val_mse: 0.1651\n",
      "Epoch 139/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0518 - mae: 0.1738 - mse: 0.0518 - val_loss: 0.0776 - val_mae: 0.2193 - val_mse: 0.0776\n",
      "Epoch 140/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0633 - mae: 0.1947 - mse: 0.0633 - val_loss: 0.0721 - val_mae: 0.2090 - val_mse: 0.0721\n",
      "Epoch 141/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0535 - mae: 0.1778 - mse: 0.0535 - val_loss: 0.0895 - val_mae: 0.2394 - val_mse: 0.0895\n",
      "Epoch 142/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0516 - mae: 0.1759 - mse: 0.0516 - val_loss: 0.0696 - val_mae: 0.2030 - val_mse: 0.0696\n",
      "Epoch 143/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0536 - mae: 0.1781 - mse: 0.0536 - val_loss: 0.1110 - val_mae: 0.2739 - val_mse: 0.1110\n",
      "Epoch 144/500\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.0658 - mae: 0.1983 - mse: 0.0658 - val_loss: 0.0944 - val_mae: 0.2435 - val_mse: 0.0944\n",
      "Epoch 145/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0548 - mae: 0.1770 - mse: 0.0548 - val_loss: 0.0974 - val_mae: 0.2532 - val_mse: 0.0974\n",
      "Epoch 146/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0549 - mae: 0.1784 - mse: 0.0549 - val_loss: 0.0749 - val_mae: 0.2114 - val_mse: 0.0749\n",
      "Epoch 147/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0731 - mae: 0.2061 - mse: 0.0731 - val_loss: 0.1669 - val_mae: 0.3482 - val_mse: 0.1669\n",
      "Epoch 148/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0560 - mae: 0.1812 - mse: 0.0560 - val_loss: 0.0987 - val_mae: 0.2574 - val_mse: 0.0987\n",
      "Epoch 149/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2631 - mae: 0.3034 - mse: 0.2631 - val_loss: 0.4399 - val_mae: 0.5255 - val_mse: 0.4399\n",
      "Epoch 150/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2203 - mae: 0.3694 - mse: 0.2203 - val_loss: 0.2914 - val_mae: 0.4367 - val_mse: 0.2914\n",
      "Epoch 151/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1908 - mae: 0.3376 - mse: 0.1908 - val_loss: 0.3186 - val_mae: 0.4452 - val_mse: 0.3186\n",
      "Epoch 152/500\n",
      "36/36 [==============================] - ETA: 0s - loss: 18.7737 - mae: 2.3344 - mse: 18.7737ETA: 0s - loss: 21.9297 - mae: 1.6647 - mse: 21.9 - 0s 8ms/step - loss: 15.8262 - mae: 2.0499 - mse: 15.8262 - val_loss: 0.8642 - val_mae: 0.7875 - val_mse: 0.8642\n",
      "Epoch 153/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1615 - mae: 0.3104 - mse: 0.1615 - val_loss: 0.1398 - val_mae: 0.2972 - val_mse: 0.1398\n",
      "Epoch 154/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1190 - mae: 0.2613 - mse: 0.1190 - val_loss: 0.2576 - val_mae: 0.4354 - val_mse: 0.2576\n",
      "Epoch 155/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1058 - mae: 0.2450 - mse: 0.1058 - val_loss: 0.1345 - val_mae: 0.2896 - val_mse: 0.1345\n",
      "Epoch 156/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0881 - mae: 0.2241 - mse: 0.0881 - val_loss: 0.1303 - val_mae: 0.2827 - val_mse: 0.1303\n",
      "Epoch 157/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0975 - mae: 0.2372 - mse: 0.0975 - val_loss: 0.3392 - val_mae: 0.5164 - val_mse: 0.3392\n",
      "Epoch 158/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1284 - mae: 0.2763 - mse: 0.1284 - val_loss: 0.3872 - val_mae: 0.5128 - val_mse: 0.3872\n",
      "Epoch 159/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1116 - mae: 0.2476 - mse: 0.1116 - val_loss: 0.2802 - val_mae: 0.4615 - val_mse: 0.2802\n",
      "Epoch 160/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0993 - mae: 0.2388 - mse: 0.0993 - val_loss: 0.0931 - val_mae: 0.2388 - val_mse: 0.0931\n",
      "Epoch 161/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0990 - mae: 0.2374 - mse: 0.0990 - val_loss: 0.1243 - val_mae: 0.2832 - val_mse: 0.1243\n",
      "Epoch 162/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0945 - mae: 0.2328 - mse: 0.0945 - val_loss: 0.0932 - val_mae: 0.2424 - val_mse: 0.0932\n",
      "Epoch 163/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0826 - mae: 0.2176 - mse: 0.0826 - val_loss: 0.1118 - val_mae: 0.2662 - val_mse: 0.1118\n",
      "Epoch 164/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0905 - mae: 0.2289 - mse: 0.0905 - val_loss: 0.1695 - val_mae: 0.3398 - val_mse: 0.1695\n",
      "Epoch 165/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1095 - mae: 0.2552 - mse: 0.1095 - val_loss: 0.0988 - val_mae: 0.2504 - val_mse: 0.0988\n",
      "Epoch 166/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0887 - mae: 0.2214 - mse: 0.0887 - val_loss: 0.0988 - val_mae: 0.2455 - val_mse: 0.0988\n",
      "Epoch 167/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.0938 - mae: 0.2334 - mse: 0.0938 - val_loss: 0.1128 - val_mae: 0.2687 - val_mse: 0.1128\n",
      "Epoch 168/500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1819 - mae: 0.3152 - mse: 0.1819 - val_loss: 0.5630 - val_mae: 0.6559 - val_mse: 0.5630\n",
      "Epoch 169/500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.8644 - mae: 0.6379 - mse: 0.8644 - val_loss: 0.5061 - val_mae: 0.5276 - val_mse: 0.5061\n",
      "Epoch 170/500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2262 - mae: 0.3751 - mse: 0.2262 - val_loss: 0.3497 - val_mae: 0.4636 - val_mse: 0.3497\n",
      "Epoch 171/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2084 - mae: 0.3586 - mse: 0.2084 - val_loss: 0.3313 - val_mae: 0.4527 - val_mse: 0.3313\n",
      "Epoch 172/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2055 - mae: 0.3557 - mse: 0.2055 - val_loss: 0.3245 - val_mae: 0.4488 - val_mse: 0.3245\n",
      "Epoch 173/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2047 - mae: 0.3548 - mse: 0.2047 - val_loss: 0.3213 - val_mae: 0.4468 - val_mse: 0.3213\n",
      "Epoch 174/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2071 - mae: 0.3570 - mse: 0.2071 - val_loss: 0.3228 - val_mae: 0.4477 - val_mse: 0.3228\n",
      "Epoch 175/500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2097 - mae: 0.3616 - mse: 0.2097 - val_loss: 0.3223 - val_mae: 0.4474 - val_mse: 0.3223\n",
      "Epoch 176/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2100 - mae: 0.3619 - mse: 0.2100 - val_loss: 0.3190 - val_mae: 0.4455 - val_mse: 0.3190\n",
      "Epoch 177/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2099 - mae: 0.3620 - mse: 0.2099 - val_loss: 0.3174 - val_mae: 0.4446 - val_mse: 0.3174\n",
      "Epoch 178/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2099 - mae: 0.3620 - mse: 0.2099 - val_loss: 0.3175 - val_mae: 0.4447 - val_mse: 0.3175\n",
      "Epoch 179/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2099 - mae: 0.3621 - mse: 0.2099 - val_loss: 0.3164 - val_mae: 0.4441 - val_mse: 0.3164\n",
      "Epoch 180/500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2099 - mae: 0.3619 - mse: 0.2099 - val_loss: 0.3180 - val_mae: 0.4449 - val_mse: 0.3180\n",
      "Epoch 181/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2100 - mae: 0.3622 - mse: 0.2100 - val_loss: 0.3174 - val_mae: 0.4446 - val_mse: 0.3174\n",
      "Epoch 182/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2099 - mae: 0.3621 - mse: 0.2099 - val_loss: 0.3167 - val_mae: 0.4442 - val_mse: 0.3167\n",
      "Epoch 183/500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2101 - mae: 0.3621 - mse: 0.2101 - val_loss: 0.3182 - val_mae: 0.4450 - val_mse: 0.3182\n",
      "Epoch 184/500\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.2100 - mae: 0.3622 - mse: 0.2100 - val_loss: 0.3167 - val_mae: 0.4442 - val_mse: 0.3167\n",
      "Epoch 185/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2101 - mae: 0.3620 - mse: 0.2101 - val_loss: 0.3186 - val_mae: 0.4452 - val_mse: 0.3186\n",
      "Epoch 186/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2099 - mae: 0.3621 - mse: 0.2099 - val_loss: 0.3164 - val_mae: 0.4441 - val_mse: 0.3164\n",
      "Epoch 187/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2100 - mae: 0.3623 - mse: 0.2100 - val_loss: 0.3164 - val_mae: 0.4441 - val_mse: 0.3164\n",
      "Epoch 188/500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2101 - mae: 0.3621 - mse: 0.2101 - val_loss: 0.3186 - val_mae: 0.4452 - val_mse: 0.3186\n",
      "Epoch 189/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2099 - mae: 0.3620 - mse: 0.2099 - val_loss: 0.3166 - val_mae: 0.4442 - val_mse: 0.3166\n",
      "Epoch 190/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2100 - mae: 0.3621 - mse: 0.2100 - val_loss: 0.3173 - val_mae: 0.4446 - val_mse: 0.3173\n",
      "Epoch 191/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2100 - mae: 0.3621 - mse: 0.2100 - val_loss: 0.3179 - val_mae: 0.4448 - val_mse: 0.3179\n",
      "Epoch 192/500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2099 - mae: 0.3620 - mse: 0.2099 - val_loss: 0.3168 - val_mae: 0.4443 - val_mse: 0.3168\n",
      "Best epoch: 142\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_dataset, train_labels, epochs=EPOCHS, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Select the best epoch\n",
    "val_mse_per_epoch = history.history['val_mse']\n",
    "best_epoch = val_mse_per_epoch.index(min(val_mse_per_epoch)) + 1\n",
    "\n",
    "print('Best epoch: %d' % (best_epoch,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 225.4082 - mae: 5.3892 - mse: 225.4082 - val_loss: 0.4839 - val_mae: 0.5701 - val_mse: 0.4839\n",
      "Epoch 2/13\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2400 - mae: 0.3862 - mse: 0.2400 - val_loss: 0.6685 - val_mae: 0.6812 - val_mse: 0.6685\n",
      "Epoch 3/13\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2270 - mae: 0.3766 - mse: 0.2270 - val_loss: 0.3009 - val_mae: 0.4349 - val_mse: 0.3009\n",
      "Epoch 4/13\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2125 - mae: 0.3622 - mse: 0.2125 - val_loss: 0.2940 - val_mae: 0.4303 - val_mse: 0.2940\n",
      "Epoch 5/13\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2295 - mae: 0.3749 - mse: 0.2295 - val_loss: 0.2954 - val_mae: 0.4317 - val_mse: 0.2954\n",
      "Epoch 6/13\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2297 - mae: 0.3795 - mse: 0.2297 - val_loss: 0.3085 - val_mae: 0.4396 - val_mse: 0.3085\n",
      "Epoch 7/13\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2152 - mae: 0.3644 - mse: 0.2152 - val_loss: 0.3191 - val_mae: 0.4473 - val_mse: 0.3191\n",
      "Epoch 8/13\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2070 - mae: 0.3584 - mse: 0.2070 - val_loss: 0.3158 - val_mae: 0.4437 - val_mse: 0.3158\n",
      "Epoch 9/13\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2059 - mae: 0.3576 - mse: 0.2059 - val_loss: 0.3198 - val_mae: 0.4459 - val_mse: 0.3198\n"
     ]
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "history = hypermodel.fit(train_dataset, train_labels, epochs=best_epoch, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4652 - mae: 0.4281 - mse: 0.4652\n",
      "[test loss, test accuracy]: [0.4652468264102936, 0.4281114339828491, 0.4652468264102936]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(test_dataset, test_labels)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'train_dataset: {train_dataset.columns()} \\n')\n",
    "# print(f'valid_dataset: {valid_dataset.columns()} \\n')\n",
    "# print(f'test_dataset: {test_dataset.columns()} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166.980042</td>\n",
       "      <td>5.030573</td>\n",
       "      <td>166.980042</td>\n",
       "      <td>26.591406</td>\n",
       "      <td>5.12968</td>\n",
       "      <td>26.591406</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss       mae         mse   val_loss  val_mae    val_mse  epoch\n",
       "0  166.980042  5.030573  166.980042  26.591406  5.12968  26.591406      0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min MSE: 166.98004150390625\n",
      "Min Val_MSE: 26.591405868530273\n"
     ]
    }
   ],
   "source": [
    "print(f'Min MSE: {hist.mse.min()}')\n",
    "print(f'Min Val_MSE: {hist.val_mse.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEKCAYAAADTgGjXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAq5ElEQVR4nO3de7xUdb3/8dd7ZoNbEQERLwgcL5mGqGnboOyoJUdT8dYRlWMqWqF2jh6t4yV/eETTvGSdMn95BDU1PeIvRTtRppaSl7TCNAzK1IBEvCBqigoIfH5/rDV7D5t9mbWd2Ws2+/18POaxZq31nTWfvTfMZz7r+13fpYjAzMysKwp5B2BmZj2Xk4iZmXWZk4iZmXWZk4iZmXWZk4iZmXWZk4iZmXVZLklE0g6SHpS0VNLbku6XtH26by9JcyStkPR7SXt0cJzDJT0nabmkWZK27b6fwszM8qpEtk7f+wLgB8BY4DpJjcCdQH/gTGAL4A5JxdYHkLQlMB14CzgL+BhwU7dEb2ZmADTk9L6/joh9SiuSjgV2Bg4kSRxnR8T300RxPrAv8MtWx5gAbABcGhE/krQncJyk7SPi+e74IczMertckkhErCw9l9QEbEpSgZROR72YLhely+1YN4l01HatJCJpEjAJoF+/fh/baaedPuBPkK+5i99i03592WpAY96hmFkv8cQTT7wWEUNab8+rEgFA0o7Aj4EFwGkk1cVaTdJlJXOztNs2IqYCUwGamppi9uzZXQm3buwy5V6O/NgwLjhk57xDMbNeQtLCtrbnNjpL0kjgV8Aq4DMR8RIwP909LF1unS7np69plNS3fFt7bddnDQWxeo3nPDOz/OVSiUgaDswiOY01GRgtaTRwN/AqcKqkt4EvkFQps9KXvgfMBUaRdKpfBpwjaQvgCOCR3tAfUiwUWOUkYmZ1IK9KZHtgCFAELgVuA26LiOXAeGAZ8F2ShDI+Ila3PkBauUwABgJXAk8CE7sh9tw1FMTq1U4iZpa/vDrWZ9HSh9F630PALu3sU6v1GcCMasdX74oFuRKx9db777/PokWLWL58ed6h9EqNjY0MGzaMPn36VNQ+145165qGoli9Zk3eYZjVxKJFi+jfvz/bbLMNUpvfNa1GIoKlS5eyaNEitt22smu3Pe1JD+RKxNZny5cvZ/DgwU4gOZDE4MGDM1WBTiI9kEdn2frOCSQ/WX/3TiI9kEdnmVm9cBLpgVyJmNVOqS+m9WObbbbJdJwpU6YgiTvuuCPT62688cZ13vvwww/PdIzu5I71Hsh9Ima1873vfY933nmHmTNncuutt3LKKaewzz770K9fv7XarVq1ioaG9j9CjzzySHbaaSfGjBnTpTiOPfZYxo0bB8CwYcPabNNWDJ3F1drq1aspFteZ47ZirkR6oKQS8egss1o45JBDOOaYY/joRz8KwOjRoznmmGPo378/kjjooIP4+Mc/zpgxY/jjH//IyJEj2WijjRg4cCAHHXQQL76YTOd3xx13MGHCBB5//HEg6WvYYYcdOPHEExkwYAD7778/7777brtxfPjDH2bs2LGMHTuWPfZI7ogxceJEJHHqqaey9dZbc/XVV7e57YUXXuDwww9n0KBBDB06lDPOOIMVK1YASaXVr18/vvzlLzNgwACefvrpD/T7ciXSAxULYpUvNrRe4MKfzGXe4reqesyRQzf5QPPO/eIXv+DCCy9kxIgR9O3blxNOOIHBgwezYMECLr30UqZMmcK0adPafO1zzz3HEUccwSc+8Qnuvfde7rzzTo477rg2215wwQVccMEFzc+nTJnSvO/hhx/mwgsvZJddduGpp55aZ9uxxx7Lo48+ysUXX8xf/vIXvvvd77LJJptw0UUXAfDuu++yePFirrzySjbffPMu/y7ASaRHaiiKFe+7EjHLw7hx4/ja174GwNNPP83//M//MGfOnOb9HX2z32qrrbjiiiuYPn069957LwsWLGi37aRJkxg/fjwA22233Vr7vvGNb3DooYcCcM0116y1bdmyZTz88MN88pOf5Gtf+xorVqzg5ptv5p577mlOIgA33XQTAwYMyPbDt8FJpAdKRmetMxOM2XqnHmeqHjp0aPPzSy65hDlz5nDhhRcyZswYxo0b1+E1FptuuilAc5/F6tXt/z/eYYcdGDt2bKcxtN4WkZyl6Giobr9+/aqSQMBJpEfy6Cyz+lD6wF62bBl33XUX77//ftWO/dRTTzF9+nQABg0axAEHHFDR6/r378/ee+/No48+ymWXXcazzz7LmjVrOOigg6oWWzknkR7Io7PM6sPkyZOZM2cOP/jBDzj55JOr9u0e4NZbb+XWW28FYLfddqs4iQDccsstnHbaaVx22WVsuOGGnH766Zx33nlVi62cSpm0t1gfbkp16i1P8PySZdx35j6dNzbrYf70pz/xkY98JO8werW2/gaSnoiIptZtPcS3B3IlYmb1wkmkB3KfiJnVCyeRHqhYKPg6ETOrC04iPZArETOrF04iPVCx6D4RM6sPuSQRSVdJekVSSJqZbpuYrrd+bNPOMVq3u7s7f4Y8ee4sM6sXeVYi01ut/wqYkD6OA1YCrwAvdnCMO8tec2UNYqxLHp1lVjujR4+mUCg0T6QIcPPNNyOJyZMnt/u6BQsWIKl55t229pU/Bg4cWIvwu10uFxtGxOlphXF62bb5wHwASUcCfYEbIqKjS0DnAT+JiHdqGG7dcZ+IWe0cddRR/Pa3v2XGjBmcdtppANx5550AHH300R/o2Lvvvjtnn302AH379m2zTTWmd8/a/oOo1z6Rk4E1wNRO2k0GlklaKGnd9L+e8p0NzWrnqKOOWutmUsuWLeO+++5jp512YpdddmH8+PEMGjSIxsZGRo4cyV133VXxsYcMGdI8vft+++0HtNyE6uijj2bnnXfmqKOOanPbihUrOPPMMxk6dCgDBw7ksMMO44UXXgDaniK+u9TdtCeStgf2A+6JiAUdNL0ceBwYAnwLuE3SFhGxzgT9kiYBkwBGjBhR9Zi7mysR6zXuORde/mD3u1jHlrvAgZe1u3v48OGMGTOGRx55hFdeeYUHH3yQ5cuXN1che+65J/vvvz/Lli1j2rRpHH/88SxZsqSit77vvvsYMmQIAPvssw+zZs1q3nfvvfdy0UUXMWLECN588811tl1yySV85zvf4YQTTmDHHXdk8uTJvPHGGzz00EPNxyifDr671F0SIalCBFxTvlFSI7AmIlYCRMS5Zfs+C3wOGA480/qAETGVtKppamrq8Z++xTSJRESHM3WaWdccffTRPPbYY8yYMYMHHngASCqU1atXM2/ePG677TZWrlzZ3H7BggU0NjZ2etzRo0dz8cUXA8mkiuVOOukkTj89OcN/4403rrPt4osvplAocO2117LBBhswc+ZMHn74YZYtW9Z8jPIp4rtLLklE0sHAqHR1uKQvknSsLwQmAn8DftbqZe8Bc4FRkg4CPg/MAgYBBwJLSPtU1ncNhSRxrF4TNBSdRGw91kHFUEvjx4/nzDPP5Ic//CFz5sxh1KhRjBw5kp///OfcdNNN7Lfffpxxxhn893//Nz/96U9Zvnx5RUlks80269L07pXK2r4a8qpEzgJKswfuCkwDTgQ+RnJ66vyI6GgM60JgK+AKoAjMBr5aqlLWd8U0caxaEzR0/dbIZtaOoUOH8qlPfYqHH34YSKoQaJn6/d1332XBggU8+uijmY67ePHi5undAf75n/+54tcefPDBPPHEE5x66qnsuOOOPP744+y9995svPHGmWKotrxGZ+3bwe7WQ39Lr1HZ87nAp6scVo9RXomYWW0cffTR6ySR/fffn2OOOYaZM2cyY8YMDjjgAG6//faKj/nkk08yYcKE5vU33nij4teed955/P3vf+f2229nxowZjBs3rls70NvjqeB7oOsfmc/XZ87jDxfsz4AN++QdjllVeSr4/GWZCr6iSkTSDRW+9xUR8ecK21oXuRIxs3pR6emsiRW0CeAWwEmkxoqFUp+Ipz4xs3xludjwmIgotPUANicZlmvdwJWIre9622n2epL1d19pEjkR+E0H+99K28zN9O7WJc2ViO8pYuuhxsZGli5d6kSSg4hg6dKlFQ1XLqnodFZE3NTJ/pVAh22sekrXhrgSsfXRsGHDWLRoUcVXgVt1NTY2MmzYsIrbV9qxPiQilqTPdyp1nktqAI4GtgOeB37UyYSJVgXFQlJAev4sWx/16dOHbbfdNu8wrEKdns6SdD3wsqS706RxX9nu24CLgf2Bb6TrVmPuEzGzelFJJfI5kgkRJwI3snYH+ieB4RGxRlIReKHaAdq6PDrLzOpFJUnkLeAR4GGS+azKJ2eZBzwq6W/AiHTdasyViJnVi0pGZ00GhkXEamA8MKVs35HADOB14K503WqspRJxEjGzfHVaiUTED8tWtwY+JWkx0FjWZtMaxGbtaEg71l2JmFnest7Z8FpgDLAlsAwYiPtBup2vEzGzepE1iexOMv16ACeRjMx6vNpBWcd8nYiZ1Yuu3GN9cbo8BBiG+0G6nUdnmVm9yHo/kWdJ+kUeA04jqUh+V+2grGMenWVm9SJrEtkfWANcD/x7uu2qqkZknfLoLDOrF1mTyNYR8Yf0+bnVDsYq49FZZlYvsvaJPClpjqSzJQ2vSUTWKVciZlYvsiaRbwH9gMuA+ZIelHRS1jeVdJWkVySFpJll2xek20qPpzo4xuGSnpO0XNIsSb1mxraWPhF3rJtZvjIlkYg4KyK2B5qA64C9galdfO/p7Wx/CJiQPs5pq4GkLdPXvwWcBXyMXjQVva8TMbN6kalPRNJg4AiSYb2fJpmM8W9Z3zQiTpe0DXB6G7vnAz+NiLc7OMQEYAPg0oj4kaQ9geMkbR8Rz2eNp6fxdSJmVi+yns56meSq9SbgB8DeEVHt00jHA29JelXSF9ppU3rPF9PlonS5XVuNJU2SNFvS7PXhRjfuEzGzepE1idxNUolsFRGnRMQjVY5nGnAUcBywEri2wr6O0vT0bX6qRsTUiGiKiKYhQ4ZUJ9IcFeVKxMzqQ6bTWRExXtIgYKySD7LHI+KNagUTEZeUnkvaHfgK8GGSTvxGYE16K975abPSPRy3Tpel7es1D/E1s3qRtU9kL+DHwKB00+uSDo2IxzIe52BgVLo6XNIXgd+Q3B3xnjSu44H3gKfTdu8Bc9PXTScZIXaOpC1IqqNHekN/CEDRfSJmVieyns76NvA+cCnJh/j7wH914X3PSl8PsCvJaaxxQBG4KN23EDgiIha3fnFEvETSuT4QuBJ4kuTOi71Cg/tEzKxOZL1ifWfgzIiYBiBpIcm1I5lExL7t7Lq0g9eo1foMkhti9TpFXydiZnUiaxJZDBwvqXTa6DhaZvW1blLqWHclYmZ5y5pEvkkyxPf+dF3Al6oakXWqUBAFuU/EzPKXdXTWNEnPAQelm34WEQ9WPyzrTEOh4ErEzHKXqWNdUh+SfpFt08dH0m3WzYoFuRIxs9xlPZ11PXAsLRf3HQGMBk6oZlDWuYaCPHeWmeUu6xDfQ4C7gA+RXAT4Y+DQagdlnSsW5dFZZpa7rJXIQ8BjEfFXAEm/pp2pRqy2Ggpyn4iZ5a6iJCLpf9OnmwCXSipVH58AHq5FYNYx94mYWT2otBIZ12r9H8ue71udUCwLj84ys3pQaRLpNXcN7ClciZhZPag0iXwPuJy27zQYwGFVi8gq4j4RM6sHWU5n3cq6p7XAHeu5SCoRj84ys3xlOZ21BJ/WqhtFXydiZnWgoiQSEQvTpws7bGjdpqHoPhEzy19FFxtK+qukgzrYPzBt84nqhWYdKXp0lpnVgUpPZ20DTJTU1M7+jYB/ADasRlDWuQaPzjKzOpDlivUj04fVgWJBrHLHupnlrNrXibzc1UAsm4aCeH+1k4iZ5auiPpGIWFjhY0Ulx5N0laRXJIWkmem2HSQ9KGmppLcl3S9p+w6OEa0ed1f0E68nir5OxMzqQNYJGKtpOnB62frWJEntApIZgk8DrgM+3cEx7gTuSJ8vqkGMdct9ImZWD3JJIhFxuqRtWDuJ/Doi9imtSDqW5AZYHZkH/CQi3ql+lPWtWCj4OhEzy13F9xORVJT0kqQv1iKQiFhZ9l5NwKYkU893ZDKwTNJCSW1dTV863iRJsyXNXrJkSXUCzpkrETOrBxUnkYhYDfwRaLefohok7Uhys6sFJKe02nM58DlgEjAIuE3SRm01jIipEdEUEU1DhgypcsT5KBY9OsvM8pf1dNZGwNmS/glYnG6LiKjKBIySRgIPACuAz0TES2X7GoE1pYolIs4t2/dZkoQyHHimGrHUO1ciZlYPsiaR0hXpe6QP6MIEjJIOBkalq8PTU2TPkHSUb0pymmq0pNERMT1t9x4wFxiVXj3/eWAWSRVyIMncXvOzxtJTeXSWmdWDrEmkWhMwngWUOtF3BaYBJwKlc02XlrWdzroWAlsBVwBFYDbw1fJ+lfWdKxEzqweZkkhELJQ0CBiTbno8It7I+qYRsW87u27s4DUqez6Xjof+rvc8d5aZ1YNMSUTSXiSd3oPSTa9LOjQiHqt6ZNYhVyJmVg8qHp2V+jbwPsnppsvS5/9V7aCsc8n9RDw6y8zylbVPZGfgzIiYBiBpIfCtqkdlnXIlYmb1IGsSWQwcL+n5dP04Wob6WjdKrhNxEjGzfGVNIt8ErgXuT9cFfKmqEVlFXImYWT3IOjprmqTngNJdDn8WEQ9WPyzrTGl0VkQgqfMXmJnVQMVJRFKRZKbc8yPirNqFZJVoKCSJY01A0TnEzHJSd3NnWWWKaRLx/Flmlqe6mjvLKleqRNwvYmZ5ymXuLPvgWioR//rNLD95zZ1lH1BzJeIbU5lZjjLdlAp4HBjb+t7qtQvP2lMsJn86VyJmlqeudKx/qHbhWKXcJ2Jm9cAd6z2UR2eZWT1wx3oP5UrEzOqBO9Z7KI/OMrN6kPmmVOXraWd7v6pGZBVpKCTdWa5EzCxPFXWsS3pd0mGSNpH0gKTd011HApnvbGgfXHMl4iG+ZpajSkdnDQQ2APoA+9JyZ8MukXSVpFckhaSZZdv3kjRH0gpJv5e0RwfHOFzSc5KWS5olqVedanOfiJnVgyx3Nox2nnfV9PIVSY3AnUB/4ExgC+CO9JQZrdpumb7+LeAs4GPATVWIqccoFj06y8zylyWJnAPcQpJALpH0vyQf4JlFxOmse1vdA0kSx/cj4vvA9SQd+fu2cYgJJJXRpRHxPeAu4B8l9ZrJIV2JmFk9yNKxXn5qaUzZ82p9ipVOR72YLhely+2AX2Zo+3yrtkiaBEwCGDFiRDVizZ1HZ5lZPag0ieTR31C6S0Yln5Idto2IqcBUgKampvXiU9ejs8ysHlSURLppfqz56XJYuty6fHvaZ7ImIlZ21rY3cCViZvUg68WGVSHpYGBUujpc0heB3wCvAqdKehv4ArAAmJW2ew+Ym75uOnAZcI6kLYAjgEciYp1TWeurYnOfiDvWzSw/WTrWq+kskiQAsCswjWSE1XhgGfBdkoQyPp34cS0R8RJJ5/pA4ErgSWBirYOuJw2+TsTM6kDmSkRSX2AkMD8i/t6VN42IfTvYvUs7r1Gr9RnAjK68//qg2HyPdScRM8tPpkokvVL9eWA2sKekP0maVpPIrEMN7hMxszqQ9XTW1cA7JKOh1pBcNzK22kFZ54q+TsTM6kDWJLIbcGPZ+mJg86pFYxUrDfF1n4iZ5Slrn8giYJ/0+a4kndsLqhmQVaY07YkrETPLU9YkcgVwXfr8WySntSZWMyCrjPtEzKweZL2fyA2SngcOJkkgMyPiVzWJzDrk60TMrB5UnETS2XQXAedHxNm1C8kq4UrEzOpBxR3r6UV/fwR6zUy59cyjs8ysHmTtE9kIOFvSP5GMzAKIiDisumFZZ5pHZzmJmFmOsiaRT6TLPWiZGt6fYjlwJWJm9SBrEulVt6CtZ547y8zqQdbRWd0xJbxVoFAQkkdnmVm+ss6d9RFJ90paLOn19LG0VsFZxxoKcp+ImeUq67Qn15LcGndLkinbB9Jya1rrZsWC3CdiZrnKmkR2J7lqPYCTgIuBx6sdlFWmoVBwJWJmuerKTalKQ3sPIbk97ZHVC8eycCViZnnLOjrrWZL7mT8GnJZu+21VI7KKJX0i7lg3s/xkTSL7k9xH5HrgdJL5s66qdlBWGVciZpa3rKezRgKjgB2Ae4CfAR+qZkCSJkqKNh7btNG2dZu7qxlLvWsoyNeJmFmuslYis2j7CvXiBw+l2a9I7lMCSXzXA28AL7bT/k7gjvR5rxopViy6EjGzfGVNIt+nJYkMAsYBj1YzoIiYD8wHkHQk0Be4ISLeb+cl84CfRMQ71YyjJ/DoLDPLW9Yr1v+tfF3SBODf2mleDSeT9MFM7aDNZOB8SX8D/jUiZrZuIGkSMAlgxIgRtYgzF+4TMbO8ZUoikso70RuAfYGh1Qyo7L22B/YD7omIBe00u5zkOpUhJHdavE3SFhHxbnmjiJhKmoiamprWm09dj84ys7xlPZ3VVtXxzWoE0oaTSUZ/XVPaIKkRWBMRKwEi4tyyfZ8FPgcMB56pUUx1xZWImeUtaxL5dNnz1cDCiHihivEAIKkvyb3b/0YyAqzkPWAuMErSQcDnSTr7BwEHAktI+1N6A8+dZWZ5y5pE/qHV+naSmlci4uYPHFHicySnqM6PiPbO1ywEtiKZhqUIzAa+WqpSegNXImaWt6xJ5EbaHuKrdHtVkkhETAemt7FdZc/nsnZl1Os0FAq+TsTMcpU1idwE/BNwA8mFihOBnwJ/qW5YVglXImaWt6xJZBQwJSKuA5C0EDglIk6uemTWqYaiWLFqdd5hmFkvljWJbAOcJmkVySms00juLWI5cCViZnnLmkQuJ+nIvp4kiQD8R1Ujsop5dJaZ5S3rFetXSrqP5CJDgFkRMafqUVlFXImYWd4qnsVX6VjeNGk8A/TBp7Jy5bmzzCxvFVUikn5JMoR3rKQvUDaXlaQLIuLiGsVnHXAlYmZ5q7QSGUUylBfglHT5dZJp279U7aCsMp47y8zyVmkSGQAslTQA2B34W0RMIbluZPMaxWadKBbEal9saGY5qrRjfQHwVWA8SeL5ebp9BLC0+mFZJRqKHp1lZvmqtBI5H9gROJgkaXwr3X4MyVTslgP3iZhZ3iqqRCLiR5IeALYD/hQRyyQ1AP8CvFzLAK19Hp1lZnmr+DqRiFhK2amriFgF/KEWQVllXImYWd4qvk7E6o9HZ5lZ3pxEejBXImaWNyeRHsxzZ5lZ3rJOwIikvUhm8y2WtlXxjoaWQbFQIALWrAkKBXX+AjOzKsuURCTdAkwo30QV72ho2TQUk8Sxak3Q10nEzHKQtRI5BHgCuBNYVf1wEpIWsPb93P8QER9to93hwJXAMJLrVU6MiPm1iqveFNPE4X4RM8tL1iTyIPBYRFxei2BaeQi4Jn3+RuudkrYkuQ/7POAs4Bsk07Ds3Q2x1YWGQqkSWUPZ2UUzs26TNYkMBi6WNI6WD/aIiMOqGxYA84GfRsTb7eyfAGwAXJpeDLkncJyk7SPi+RrEU3dciZhZ3rKOztqL5CvvXsC4skctHA+8JenVdPr51rZNly+my0XpcrvWDSVNkjRb0uwlS5bUINR8tFQiTiJmlo+slci2nTepimkkN75qBC4DrpX0QCf9HaWe5XU+USNiKuk9UJqamtabT9xiIfkO4ErEzPKS9fa4C2sVSKv3uaT0XNLuwFeAD0t6CVgTEStJTndB0qkOsHW67DUd665EzCxvWYf4fgT4DrALSZUASZ/I4GoFJGkXkk7ye9L4jgfeA55Ol3NJbpI1naRKOUfSFsARwCO9pT8EyvpEfE8RM8tJ1j6Ra4ExJPdWXwYMpKUvolpeI+l3uYgkSSwEjoiIxeWNIuIlks71gSTDfJ8EJlY5lrrWcp2I588ys3xk7RPZneSD/SLgJJLhtFtUM6A0ORzUzj61Wp8BzKjm+/ckHp1lZnnrytxZpYrgEJL+iCOrF45lUZT7RMwsX1krkWdJOrAfA04jGQn1u2oHZZVxJWJmecuaRPYH1gDXA/+ebruqqhFZxUp9Ik4iZpaXrEN8X5PUFxhBcqX432sTllWidJ2IT2eZWV4y9Ymk12w8D8wG9pQ0T9K0mkRmnWrw6Swzy1nWjvWrgXdIrg5fA9wKjK12UFaZYsFDfM0sX1mTyG7AjWXri4HNqxaNZeJKxMzylrVjfRGwT/p8V5KL/RZUMyCrXNHTnphZzrImkSuA69Ln306XJ1YvHMuioTQBo6c9MbOcZB2ddYOk54GD000zI+Kh6odllXAlYmZ5qyiJSFrdzq6vSoqIyFrRWBX4OhEzy1ulH/4iuTp9MfBmzaKxTDw6y8zyVunorBtJhvZuRjIl+1ciYpfSo1bBWcc8OsvM8lZREomIk0imf/8yMBz4uaQFkj5by+CsY+4TMbO8VXydSES8C/yV5M6BK0mqkv41issq0ODb45pZzipKIpLOk/Qs8ADwIZIZfLeKiB/VMjjrmCsRM8tbpR3rF5N0rP+V5M6DhwKHKrmfRUTEYbUJzzrS3Cey2h3rZpaPLENzBWyfPsr5a3BOikVXImaWr0qTyLY1jaKMpB2AqSTTqvQFHgdOiYjn22jb+tPzxxFxeM2DrBMenWVmeasoiUTEwloHUmZrkr6aC4APk/S/XAd8up32dwJ3pM8X1Ty6OuI+ETPLWz1eaf7riChN8oikY4GdO2g/D/hJRLxT88jqjEdnmVnesk4FX3MRsbL0XFITsCnQ0fxck4FlkhZKGlfr+OpJWoi4EjGz3NRdEimRtCPwY5Kp5k9rp9nlwOeAScAg4DZJG7VxrEmSZkuavWTJkhpF3P0k0VAQqz3tiZnlpC6TiKSRwK+AVcBnIuKldHtjeo93ACLi3Ii4OyKmAfcDG5NcUb+WiJgaEU0R0TRkyJDu+SG6SbEgVyJmlpu66xORNByYRXIaazIwWtLoiJgOvAfMBUZJOgj4fNp2EHAgsITkivpeo6Eg30/EzHJTd0mE5DqUUrlwadn26a3aLQS2IrlRVhGYDXy1vE+lN3AlYmZ5qrskEhGzSC5sbGufyp7Ppf1hv71GQ7Hg0Vlmlpu67BOxyrkSMbM8OYn0cB6dZWZ5chLp4VyJmFmenER6uKQScRIxs3w4ifRwrkTMLE9OIj1cQ6Hg60TMLDdOIj2cKxEzy5OTSA/XUPToLDPLj5NID+dKxMzy5CTSw3l0lpnlyUmkh3MlYmZ5chLp4RoKnjvLzPLjJNLDuRIxszzV3Sy+1okIWLUCVrwNK99mu/ef47WV78M7O8KGm0LB3wvMrPs4iXSH0gf/ymWw4i1YsSxNAumy9Gh3fVlz0mDF27BmVfOhLyg9+ebpUOwLG28Jm2wF/beCTYa2Wqbb+2yYy6/BzNY/TiKVen0+vP1S+oH+Vvsf8CvKEkH5tjXvV/Y+fTdOHhv0hw3S5Uabrb3ed2PYYBPYYGOu/c2rvPLmu/znPpvC24vhrZeSOF9+Gp69D95/d933aBzYKrEMXXe50WBXNWbWKSeRSv1iCsy7u+19ffqt+yE/aJtWH/r9Wx7trffdOPMH9x//9CRz3/47/zlm33V3RiQJ762XyhJMWaJ5azG8MheWvQK06lcp9IH+W3aQaNJH340yxWtm6xcnkUp96kz42AlJBVBeKfTdGArF3MJq6KhjXYLGAclj853aP8jqVUkiKSWW8uXbL8Er8+C5XybVV2uNA9pOMJsMTfpoVLoZpVpian5OG/uyrnfxtZ2qcLBCVNIuy7Eiw5KM7aMllq4ea53X0s5+2t7f4Wuy7K+Q2vp7q5M2Gfe3+z60ijcq21fxa7IeCxj1Odh487aP0UVOIpUa+tG8I2hTsSBef2clX585r/mftgSSknWBULKttK9snbRdsj4AGID0EVQADQQNUvMx+656h41XvsrGK5bQL11utHIJ/Va8Sr8lr7DRoqfZcMVSCngaFrN6tHzLPWjsDUlE0l7ANcCOwFzgixHx+zbaHQ5cCQwDHgdOjIj53Rhq7nYbNoB7577M7b97gYggSL58BJEuSb9otqyXt+uaAeljh3X2FFnNZvydLfU6A/QOkCYvyr5h0vJdTs3rlW0vf33XXxtERRVJZVVLZbVIpcdKok6Wa/8k5dvW3d9+m+bjRsv2UtzrtGnnOND6tR2vs86x214HtRQua/1M667Txv5y5S2T9XW1btP6r7duzdF6/7p/7bZes/bvofzd1m7dXjta/R7bi7iS9yl/frO2YzeqS9H1T5KakNQILADeA74J/B9gBbBDRKwua7dl2m4e8APgG8CTEbF3R8dvamqK2bNn1yT2niyi7SRTSj60Wl8nGTV/EHR8nDaPUcnx19qetFuzJkN8rV5fUOkMRFKVFdaqyNJKrex5QWtXc6D0GC2vK6ilapPS/WtVga2ek76m+b3qV/nfufx3vKb5302k+7r+N+7s30tbr6fsGKX9pT0tyansefpkrW0t/3hbHafVe7fxfq3/3Ze/NouufAp35bN79LaDGbBRny68G0h6IiKaWm+vx0rkQGAL4OyI+H6aLM4H9gV+WdZuArABcGlE/EjSnsBxkraPiOe7O+iervQBl67lGYqZ9SD1mES2TZcvpstF6XI71k4iHbVbK4lImgRMSleXSXqmi7FtBrzWxdfWkuPKxnFl47iyWV/j+oe2NtZjEmmt9LW4s9qt3XYRMRWY+oEDkWa3Vc7lzXFl47iycVzZ9La46vFqsvnpcli63Lq0XVKjpL6dtatxfGZmlqrHSuQe4FXgVElvA18g6UCfBawiGa01CpgOXAacI2kL4AjgEfeHmJl1n7qrRCJiOTAeWAZ8lyShjC8fmZW2e4mkc30gyTDfJ4GJNQ7vA58SqxHHlY3jysZxZdOr4qq7Ib5mZtZz1F0lYmZmPYeTiJmZdZmTSAUk7SVpjqQVkn4vaY+8YwKQdJWkVySFpJl5xwMgaQdJD0paKultSfdL2j7vuAAk/SaN6V1JsyV1OLtBd0pHHj6T/i2vzjueEkkL0phKj6fqIKaBkm6W9KakZZIeyjsmAEkTW/2uSo9t6iC2M9K/5QpJ8yWdVq1jO4l0Ip2G5U6gP3AmydX0d0jKb+retU3PO4BWtib5d3UByXQ0Y4Hrco2oxa+B04GvAx+lfuIC+E9ahqvXm4dIBrFMAM7JORaAG4BjgeuBM4Dnco2mxa9o+T0dB6wEXqHlguhcSNoB+C9gDfAVoA9wlaThVXmDZF4bP9p7kAwdDuCsdP2idH2/vGNL49kmjWdm3rGk8fRttb4UeDXvuNJYRHLV7seBd4A/5x1TGteuJHPFnZX+La/OO6ay2BYANwL9844ljWe79Hd0C9AXKOYdUztxHpnG+Y06iGXHNJaH0+ezgeXAkGoc35VI5zqaXsVaiYiVpeeSmoBNSb7J1oMBwBLgNyTfEr+YbzggqUBSEf1f4Hc5h9Oe44G3JL0q6Qs5xzIyXe5J8kXgHUmX5xhPe04m+eaf+3DfiHgGOBfYC/gzsDswKSKWVOP4TiLZVToNS68maUfgxyTfZKt2/vUDWgbsT3JKq5GkqszbiSTV5M20zLowQNKQ3CJa2zTgKFpOz1wraduOX1JTG6TLfsDRwKPA2ZLG5hfS2tI+wP2An0fEgpzDIf23dBrwFHA48AfgaklVOX3qJNI5T6+SkaSRJOeHVwGfieTC0NxFxKqIuD8ivgf8Fvi0pM1yDms4MITkP/Yt6bbPA5fmFlGZiLgkIu6IiFuA24Ei8OEcQ1qQLh+OiBnA/0vX62LwRupkki+b1+QdSOrTJJ9bMyLix8AMkj7eT1Tj4PU47Um96WgallxJOphkChiA4ZK+CPwqIp7NMabhJL+bTYHJwGhJoyMi1wEAkg4g+Ub9a5IP7k+SdHouzTMukg/BP6bPdwamAD+nDj6AJO1Ccp+ee0g+K44n6bt5Osewfp++/36SvkRSya0mqUhyl87tNxH4G/CzfKNp9td0+XlJL5EMSgD4S1WOnnenT094AHuT/MNdSTK9SlPeMaVxzaJ0k7iWx8ScY9q3jZiiDn5Xe5J8WL8HvAk8COyZd1zt/O7qomMd2Irkg/A14F2SDtkD6iCunYHHSDqH/wL8S94xlcV2TPo3nJx3LK3i+grJ2ZPlJEnlX6t1bE97YmZmXeY+ETMz6zInETMz6zInETMz6zInETMz6zInETMz6zInEbMqkbRNGzO4vlmD95mSHvvIah/bLCtfbGhWfU8CV6TPV3bU0KyncyViVn1LgF+kj1+W3Wfi+vR+NK9J+o9SY0lfkvSspHck/VbSp9LtfSVdKmmhpPfauG/GXpL+LGmJpPHd9+OZtXASMau+/UkSyRKSSShLPgtcC7wMfFPSbpI+QzLT6xKSq4pHAP8raTDJzKvnAnOBfyOZ8qPcgSTTowwALqvZT2PWAZ/OMqu+35DMGwbwBrBL+vyGiLhW0iqS6d/3IUkaABdExP2SRgDnAWOAQ0im0Dg6It5u432+HRFTJZ0K7FCjn8WsQ04iZtX3WkT8orSSTmRYTqyrvfmHOpqX6PV0uQqfVbCcOImYVd9QSceUrfdJlydJeoHkfiZBMl3+YOCrwIXpfShOIqleHgd+AjQBt0u6A9g1Is7onh/BrDJOImbVtztwW9n6menyZ8ApwJbA2RHxBwBJk4CzgW8D84AzI2KppMuADUmm7v4MyT1QzOqKZ/E1qzFJE4EfAGdFxJU5h2NWVT6PamZmXeZKxMzMusyViJmZdZmTiJmZdZmTiJmZdZmTiJmZdZmTiJmZddn/B0uZtLvkgB+1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [ibova_0]')\n",
    "  plt.plot(hist['epoch'], hist['mae'],label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mae'],label = 'Val Error')\n",
    "  plt.ylim([0,5])\n",
    "  plt.legend()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Square Error [$ibova_0^2$]')\n",
    "  plt.plot(hist['epoch'], hist['mse'],label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mse'],label = 'Val Error')\n",
    "  plt.ylim([0,20])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# O parâmetro patience é o quantidade de epochs para checar as melhoras\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "  monitor='val_mse', patience=25, mode='min' ,restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_dataset, train_labels, epochs=EPOCHS,\n",
    "                    validation_split = 0.2, verbose=1, callbacks=[early_stop, tensorboard_callback])\n",
    "\n",
    "# Mudar os dados de validação\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
    "\n",
    "print(f\"Testing set Mean Abs Error: {mse:5.2f} ibova_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizando as previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_dataset).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando as previsões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_labels():\n",
    "\t# Imprimindo valores reais\n",
    "\tplt.plot(test_labels, color ='r', label='ibova_REAL')\n",
    "\tplt.xlabel(\"Dates\")\n",
    "\tplt.ylabel(\"Variation ROC\")\n",
    "\tplt.title(\"ibova_REAL\")\n",
    "\tplt.legend()\n",
    "\n",
    "def show_model_predictions():\n",
    "\t# Imprimindo previsoes\n",
    "\tplt.plot(test_predictions, color ='g', label='predictions_MODEL')\n",
    "\tplt.xlabel(\"Dates\")\n",
    "\tplt.ylabel(\"Variation ROC\")\n",
    "\tplt.title(\"predictions_MODEL\")\n",
    "\tplt.legend()\n",
    "\n",
    "def show_compare_graph():\n",
    "\t# Predictt X Real values\n",
    "\tplt.plot(test_labels, color ='r', label='ibova_REAL')\n",
    "\tplt.plot(test_predictions, color ='g', label='predictions_MODEL')\n",
    "\tplt.xlabel(\"Dates\")\n",
    "\tplt.ylabel(\"Variation ROC\")\n",
    "\tplt.title(\"Predict X Real values\")\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "def show_true_predict_values():\n",
    "\tplt.figure(figsize=(24,4))\n",
    "\tplt.scatter(test_labels, test_predictions)\n",
    "\tplt.xlabel('True Values [ibova_0]')\n",
    "\tplt.ylabel('Predictions [ibova_0]')\n",
    "\tplt.axis('equal')\n",
    "\tplt.axis('square')\n",
    "\tplt.xlim([0,plt.xlim()[1]])\n",
    "\tplt.ylim([0,plt.ylim()[1]])\n",
    "\t_ = plt.plot([-100, 100], [-100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_compare_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando Medias:\n",
    "\n",
    "print(f'Massa de predição: {test_predictions.mean()}')\n",
    "print(f'Massa inicial: {ibova_test.mean()}')\n",
    "print(f'Diferenças das médias: {ibova_test.mean() - test_predictions.mean()}')\n",
    "\n",
    "# Add o RM_MSE medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pystock')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc0c8eb905859abd75b389576d87e2ac71c748b72952270660ecc130aeb3e651"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
